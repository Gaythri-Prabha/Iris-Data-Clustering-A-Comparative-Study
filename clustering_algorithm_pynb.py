# -*- coding: utf-8 -*-
"""Clustering Algorithm.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZNZJXv2Q__53BCn1Ge6Q9D7lAwVTU-xQ

# **1. Loading and Preprocessing**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.datasets import load_iris
iris_data = load_iris()
df = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names)
df.head()

iris_data.feature_names # The dataset in sklearn does not include species names in the main dataframe

df.describe()

df.info()

#"Boxplot of Iris Dataset Features (Detecting Outliers
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.boxplot(data=df)
plt.title("Boxplot of Iris Dataset Features (Detecting Outliers)")
plt.show()

sns.pairplot(df)
plt.show()  #pairplot is used for further detetcion of outliers

"""**Most features do not have extreme outliers, except for Sepal Width (cm) at 2.0 cm, which is a potential low outlier.**

### 2.Clustering Algorithm Implementation

**A) KMeans Clustering**

**K-Means is an unsupervised machine learning algorithm used for clustering.
 It groups data points into K clusters based on similarity.**



1.   Choose the number of clusters (K).


2.   Randomly initialize K centroids in the feature space.


3.   Assign each data point to the nearest centroid based on Euclidean distance.


4.   Update centroids by computing the mean of all points assigned to each cluster.

5.Repeat steps 3 and 4 until centroids no longer change significantly (convergence).
"""

from sklearn.cluster import KMeans
# Apply K-Means Clustering
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
df['cluster'] = kmeans.fit_predict(df)

#Visualize clusters using petal length & petal width
plt.figure(figsize=(10,6))
sns.scatterplot(x = df['petal length (cm)'],y=df['petal width (cm)'], hue=df['cluster'], palette='viridis')

plt.title("K-Means Clustering on Iris Dataset")
plt.xlabel("Petal Length (cm)")
plt.ylabel("Petal Width (cm)")
plt.legend()
plt.show()

from sklearn.metrics import silhouette_score

# Calculate silhouette score
silhouette_avg = silhouette_score(df.iloc[:, :-1], df['cluster'])  # Exclude 'cluster' column
print(f"Silhouette Score: {silhouette_avg:.4f}")

"""Silhouette Score of 0.5528 indicates that the K-Means clustering is performing fairly well:

 Clusters are well-separated with good cohesion.
 Some slight overlap may exist, but overall, the algorithm is forming meaningful groups.

**B) Hierarchical Clustering**

**Hierarchical Clustering is an unsupervised learning algorithm used to group data into clusters without specifying the number of clusters in advance. It builds a hierarchy (tree-like structure)**

Each data point starts as its own cluster.

The two closest clusters are merged at each step based on distance metrics (e.g., Euclidean distance).

his continues until only one cluster remains.
"""

from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

#  Creating the linkage matrix (Ward's method minimizes variance)
linkage_matrix = linkage(df.iloc[:, :-1], method='ward')


plt.figure(figsize=(12, 6))
dendrogram(linkage_matrix, labels=df.index, leaf_rotation=90, leaf_font_size=8)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()

# Apply Agglomerative Clustering with 3 clusters (since we know there are 3 species)
agg_clustering = AgglomerativeClustering (n_clusters=3, linkage='ward')
df['hierarchical_cluster'] = agg_clustering.fit_predict(df.iloc[:, :-2])  # Exclude K-Means column
# Step 4: Visualize Clusters using Petal Length & Width
plt.figure(figsize=(8, 6))
sns.scatterplot(x=df['petal length (cm)'], y=df['petal width (cm)'],
                hue=df['hierarchical_cluster'], palette='viridis', s=100, alpha=0.7)
plt.title("Hierarchical Clustering on Iris Dataset")
plt.xlabel("Petal Length (cm)")
plt.ylabel("Petal Width (cm)")
plt.show()

from sklearn.metrics import silhouette_score

# Calculate silhouette score for hierarchical clustering
hierarchical_silhouette = silhouette_score(df.iloc[:, :-2], df['hierarchical_cluster'])  # Exclude cluster columns

print(f"Silhouette Score for Hierarchical Clustering: {hierarchical_silhouette:.4f}")

"""Comparing K-Means and Hierarchical Clustering

K-Means Silhouette Score: 0.5528  (Better separation)

Hierarchical Clustering Silhouette Score: 0.5175  (Slightly lower)
```

```

Interpretation:

Both methods perform well, but K-Means gives a slightly better separation of clusters.

This suggests K-Means forms more distinct groups, while Hierarchical Clustering may have some overlap.

However, Hierarchical Clustering provides more insights into relationships between clusters via the dendrogram.
"""

